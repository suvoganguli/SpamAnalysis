{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e21d1a-2270-4b19-87aa-229896e5e41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4743.2005-06-25.GP.spam.txt\n",
      "3165.2004-12-11.GP.spam.txt\n",
      "4558.2005-05-23.GP.spam.txt\n",
      "2826.2004-11-16.GP.spam.txt\n",
      "1096.2004-05-14.GP.spam.txt\n",
      "2681.2004-10-31.GP.spam.txt\n",
      "4922.2005-07-25.GP.spam.txt\n",
      "4421.2005-04-30.GP.spam.txt\n",
      "0707.2004-03-23.GP.spam.txt\n",
      "2575.2004-10-22.GP.spam.txt\n",
      "1057.2004-05-06.GP.spam.txt\n",
      "2784.2004-11-11.GP.spam.txt\n",
      "3180.2004-12-14.GP.spam.txt\n",
      "4283.2005-04-16.GP.spam.txt\n",
      "3090.2004-12-06.GP.spam.txt\n",
      "1061.2000-05-10.farmer.ham.txt\n",
      "2618.2000-10-23.farmer.ham.txt\n",
      "2986.2000-11-30.farmer.ham.txt\n",
      "2300.2000-09-21.farmer.ham.txt\n",
      "4688.2001-06-20.farmer.ham.txt\n",
      "3819.2001-03-15.farmer.ham.txt\n",
      "2404.2000-09-29.farmer.ham.txt\n",
      "2773.2000-11-06.farmer.ham.txt\n",
      "1935.2000-08-14.farmer.ham.txt\n",
      "1928.2000-08-11.farmer.ham.txt\n",
      "3546.2001-02-07.farmer.ham.txt\n",
      "1644.2000-07-14.farmer.ham.txt\n",
      "2401.2000-09-29.farmer.ham.txt\n",
      "3194.2000-12-27.farmer.ham.txt\n",
      "3098.2000-12-14.farmer.ham.txt\n",
      "3905.2001-03-21.farmer.ham.txt\n",
      "3071.2000-12-12.farmer.ham.txt\n",
      "2921.2000-11-22.farmer.ham.txt\n",
      "3078.2000-12-12.farmer.ham.txt\n",
      "0563.2000-03-06.farmer.ham.txt\n",
      "2973.2000-11-29.farmer.ham.txt\n",
      "2485.2000-10-09.farmer.ham.txt\n",
      "1091.2000-05-19.farmer.ham.txt\n",
      "2471.2000-10-06.farmer.ham.txt\n",
      "2155.2000-09-05.farmer.ham.txt\n",
      "4582.2001-05-22.farmer.ham.txt\n",
      "1438.2000-06-21.farmer.ham.txt\n",
      "1065.2000-05-11.farmer.ham.txt\n",
      "3236.2001-01-02.farmer.ham.txt\n",
      "4747.2001-07-11.farmer.ham.txt\n",
      "2963.2000-11-28.farmer.ham.txt\n",
      "0413.2000-02-14.farmer.ham.txt\n",
      "0467.2000-02-23.farmer.ham.txt\n",
      "3197.2000-12-27.farmer.ham.txt\n",
      "0329.2000-02-04.farmer.ham.txt\n",
      "4927.2001-09-21.farmer.ham.txt\n",
      "1086.2000-05-18.farmer.ham.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron1/spam\"\n",
    "\n",
    "# List to store texts\n",
    "emails = []\n",
    "labels = []\n",
    "\n",
    "i = 0\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('spam')\n",
    "        if i%100 == 0:\n",
    "            print(filename)\n",
    "        i = i+1\n",
    "        \n",
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron1/ham\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('ham')\n",
    "        if i%100 == 0:\n",
    "            print(filename)\n",
    "        i = i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bb121e-5b56-42c2-b807-c494f34c41e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2913.2005-06-28.SA_and_HP.spam.txt\n",
      "3434.2005-07-01.SA_and_HP.spam.txt\n",
      "0905.2002-07-24.SA_and_HP.spam.txt\n",
      "3910.2005-07-04.SA_and_HP.spam.txt\n",
      "1217.2002-08-23.SA_and_HP.spam.txt\n",
      "2441.2005-06-25.SA_and_HP.spam.txt\n",
      "0629.2002-07-20.SA_and_HP.spam.txt\n",
      "0355.2002-05-27.SA_and_HP.spam.txt\n",
      "3893.2005-07-04.SA_and_HP.spam.txt\n",
      "3963.2005-07-04.SA_and_HP.spam.txt\n",
      "3088.2005-06-29.SA_and_HP.spam.txt\n",
      "0262.2002-05-20.SA_and_HP.spam.txt\n",
      "4111.2005-07-05.SA_and_HP.spam.txt\n",
      "3438.2005-07-01.SA_and_HP.spam.txt\n",
      "5623.2005-07-20.SA_and_HP.spam.txt\n",
      "4769.2001-03-16.kaminski.ham.txt\n",
      "1328.2000-05-25.kaminski.ham.txt\n",
      "5793.2001-05-04.kaminski.ham.txt\n",
      "1501.2000-06-20.kaminski.ham.txt\n",
      "1866.2000-07-25.kaminski.ham.txt\n",
      "1450.2000-06-14.kaminski.ham.txt\n",
      "3291.2000-11-10.kaminski.ham.txt\n",
      "3552.2000-12-04.kaminski.ham.txt\n",
      "3944.2001-01-07.kaminski.ham.txt\n",
      "2862.2000-10-18.kaminski.ham.txt\n",
      "3617.2000-12-11.kaminski.ham.txt\n",
      "2146.2000-08-11.kaminski.ham.txt\n",
      "4958.2001-04-03.kaminski.ham.txt\n",
      "4791.2001-03-19.kaminski.ham.txt\n",
      "0381.2000-02-05.kaminski.ham.txt\n",
      "1844.2000-07-21.kaminski.ham.txt\n",
      "4547.2001-02-26.kaminski.ham.txt\n",
      "0664.2000-03-08.kaminski.ham.txt\n",
      "4497.2001-02-16.kaminski.ham.txt\n",
      "5714.2001-05-02.kaminski.ham.txt\n",
      "0299.2000-01-24.kaminski.ham.txt\n",
      "3725.2000-12-18.kaminski.ham.txt\n",
      "1720.2000-07-07.kaminski.ham.txt\n",
      "5049.2001-04-09.kaminski.ham.txt\n",
      "2490.2000-09-18.kaminski.ham.txt\n",
      "0638.2000-03-06.kaminski.ham.txt\n",
      "4552.2001-02-26.kaminski.ham.txt\n",
      "2097.2000-08-09.kaminski.ham.txt\n",
      "1277.2000-05-19.kaminski.ham.txt\n",
      "0896.2000-04-04.kaminski.ham.txt\n",
      "4458.2001-02-12.kaminski.ham.txt\n",
      "2970.2000-10-24.kaminski.ham.txt\n",
      "3803.2000-12-21.kaminski.ham.txt\n",
      "0666.2000-03-08.kaminski.ham.txt\n",
      "1166.2000-05-02.kaminski.ham.txt\n",
      "5490.2001-04-26.kaminski.ham.txt\n",
      "0004.1999-12-10.kaminski.ham.txt\n",
      "0286.2000-01-21.kaminski.ham.txt\n",
      "4390.2001-02-02.kaminski.ham.txt\n",
      "2515.2000-09-19.kaminski.ham.txt\n",
      "0356.2000-02-01.kaminski.ham.txt\n",
      "0017.1999-12-14.kaminski.ham.txt\n",
      "0392.2000-02-08.kaminski.ham.txt\n",
      "2237.2000-08-18.kaminski.ham.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron2/spam\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('spam')\n",
    "        if i%100 == 0:\n",
    "            print(filename)\n",
    "        i = i+1\n",
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron2/ham\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('ham')\n",
    "        if i%100 == 0:\n",
    "            print(filename)\n",
    "        i = i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa91dda-dbb5-4c47-b050-58e11ea7bdbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 7\u001b[0m         email \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Read the content of the file\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         emails\u001b[38;5;241m.\u001b[39mappend(email)  \u001b[38;5;66;03m# Append the text to the list\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron3/spam\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('spam')\n",
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron3/ham\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('ham')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24398661-7610-4e15-a672-288ef16f59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron4/spam\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('spam')\n",
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron4/ham\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('ham')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b23e2-a40d-4397-bb60-de8bda17f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron5/spam\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('spam')\n",
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron5/ham\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('ham')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ae687-4348-4e05-945a-80d48133c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron6/spam\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('spam')\n",
    "\n",
    "# Directory containing text files\n",
    "directory = \"enron6/ham\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), \"r\", encoding=\"latin-1\") as file:\n",
    "        email = file.read()  # Read the content of the file\n",
    "        emails.append(email)  # Append the text to the list\n",
    "        labels.append('ham')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c65f52-92a8-4a61-abce-4ff9e65eac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Email Label\n",
      "0      Subject: what up , , your cam babe\\nwhat are y...  spam\n",
      "1      Subject: want to make more money ?\\norder conf...  spam\n",
      "2      Subject: food for thoughts\\n[\\njoin now - take...  spam\n",
      "3      Subject: miningnews . net newsletter - tuesday...  spam\n",
      "4      Subject: your pharmacy ta\\nwould you want chea...  spam\n",
      "...                                                  ...   ...\n",
      "11024  Subject: f / u from iris mack , mba / phd to e...   ham\n",
      "11025  Subject: re : vacation\\nshirley ,\\nno problem ...   ham\n",
      "11026  Subject: ibuyit form\\nattached please find the...   ham\n",
      "11027  Subject: vince kaminski ' s discussion notes f...   ham\n",
      "11028  Subject: re : visit to enron\\nfyi\\n- - - - - -...   ham\n",
      "\n",
      "[11029 rows x 2 columns]\n",
      "0        Subject: what up , , your cam babe\\nwhat are y...\n",
      "1        Subject: want to make more money ?\\norder conf...\n",
      "2        Subject: food for thoughts\\n[\\njoin now - take...\n",
      "3        Subject: miningnews . net newsletter - tuesday...\n",
      "4        Subject: your pharmacy ta\\nwould you want chea...\n",
      "                               ...                        \n",
      "11024    Subject: f / u from iris mack , mba / phd to e...\n",
      "11025    Subject: re : vacation\\nshirley ,\\nno problem ...\n",
      "11026    Subject: ibuyit form\\nattached please find the...\n",
      "11027    Subject: vince kaminski ' s discussion notes f...\n",
      "11028    Subject: re : visit to enron\\nfyi\\n- - - - - -...\n",
      "Name: Email, Length: 11029, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with the texts\n",
    "df = pd.DataFrame({\"Email\": emails, \"Label\": labels})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "X = df['Email']\n",
    "y = df['Label']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d722b4-ff2d-4e41-a41e-14ee6e1b318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      1608\n",
      "        spam       0.98      0.96      0.97       598\n",
      "\n",
      "    accuracy                           0.99      2206\n",
      "   macro avg       0.99      0.98      0.98      2206\n",
      "weighted avg       0.99      0.99      0.99      2206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = nb_classifier.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example \n",
    "\n",
    "# new_emails = [\"New email text 1\", \"New email text 2\", ...]\n",
    "# new_emails_vec = vectorizer.transform(new_emails)\n",
    "# predictions = nb_classifier.predict(new_emails_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29cc6c21-9a32-41e8-9ff6-c7dfa81e5ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.985040797824116\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      1608\n",
      "        spam       0.97      0.97      0.97       598\n",
      "\n",
      "    accuracy                           0.99      2206\n",
      "   macro avg       0.98      0.98      0.98      2206\n",
      "weighted avg       0.99      0.99      0.99      2206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression(max_iter = 10000)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb47097f-5f19-43e4-baea-b6d9f23d4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.970988213961922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      0.99      0.98      1608\n",
      "        spam       0.97      0.92      0.95       598\n",
      "\n",
      "    accuracy                           0.97      2206\n",
      "   macro avg       0.97      0.96      0.96      2206\n",
      "weighted avg       0.97      0.97      0.97      2206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a7cae6-c574-48c1-9362-fe94f514b283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 17:40:13.780276: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype variant and shape [8823,3]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-04-20 17:40:13.780512: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [8823]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-04-20 17:40:14.482703: W tensorflow/core/framework/op_kernel.cc:1807] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/d7/3y4pn1x55_583bts49jyqlxh0000gn/T/ipykernel_1587/2375070632.py\", line 29, in <module>\n      history = model.fit(X_train_vectorized, y_train, epochs=5, batch_size=32)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/losses.py\", line 2145, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_5272]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vectorized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_vectorized, y_test)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/d7/3y4pn1x55_583bts49jyqlxh0000gn/T/ipykernel_1587/2375070632.py\", line 29, in <module>\n      history = model.fit(X_train_vectorized, y_train, epochs=5, batch_size=32)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/keras/losses.py\", line 2145, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_5272]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert text data into numerical vectors using TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Sort the indices of the sparse matrices\n",
    "X_train_vectorized.sort_indices()\n",
    "X_test_vectorized.sort_indices()\n",
    "\n",
    "# Define a neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_vectorized.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_vectorized, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_vectorized, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Predict on new data\n",
    "new_emails = [...]  # Your new emails to classify\n",
    "new_emails_vectorized = vectorizer.transform(new_emails)\n",
    "new_emails_vectorized.sort_indices()  # Ensure indices are sorted\n",
    "predictions = model.predict(new_emails_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d531a8db-360f-4759-9ba8-40f9f3603d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8823,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b3af3-bd97-4451-8dd2-8ea8f7950471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
